{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ds8gIcDzSmfI"
   },
   "source": [
    "# Sentiment Analysis of Text Based Reviews using RNN with LSTM cells\n",
    "Tejas Pant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3Qln8cAVRNd"
   },
   "source": [
    "### Import all the required APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pzS4D5Tn7d4a",
    "outputId": "002769ef-5eb2-4089-fd81-860c6cb8d2bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from google.colab import files\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from google.colab import files\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SyntaxError('GPU not found')\n",
    "\n",
    "print('GPU:{}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uLSrkAtEVmzz"
   },
   "source": [
    "### Load Data and Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvHUgZeC8UYb"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "#uploaded = files.upload()\n",
    "with open('reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tujJZBB79wfE"
   },
   "outputs": [],
   "source": [
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()\n",
    "\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "reviews_ints = []\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "    \n",
    "# Creating dictionary\n",
    "vocab = sorted(set(words))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab,1)}\n",
    "\n",
    "# Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "\n",
    "reviews_ints = []\n",
    "for review in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VGhrrCyxYEL8",
    "outputId": "175122db-96f0-4dc6-ece9-bc0b2e95661e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "labels = labels.split('\\n')\n",
    "labels = np.array([1 if each == 'positive' else 0 for each in labels])\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmAkTY6nYgj7"
   },
   "outputs": [],
   "source": [
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "labels = np.array([labels[ii] for ii in non_zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77qqGIdC-yrj"
   },
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "features = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-LkmsVZ3-4o2",
    "outputId": "882073cc-52f8-47ad-a304-b120b649fdbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,  8210, 29951, 33665,\n",
       "            1,  9819, 12477, 33768, 52591,  3689, 65543, 56451, 66137,\n",
       "         3397, 60807, 46549, 51107,   181, 57199, 37775, 63328,  3397,\n",
       "        64947, 43774, 73400, 32081, 65543, 64949, 51048, 37169, 40892,\n",
       "        66340,  5656, 65523,  8210, 29951, 56131, 56749, 33665, 43370,\n",
       "        11909],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0, 62632, 45809,     1, 39682,\n",
       "        72144, 29037, 69126, 23140, 24476,     1, 49065, 62117, 46592,\n",
       "        72586],\n",
       "       [30468, 46281, 30894,  3397, 26255,  9670, 62132, 29037,  5480,\n",
       "         2085, 33756, 24476, 73400,  8855, 44548,     1, 49379, 66340,\n",
       "        29594, 65827, 46049, 65543, 62781, 65523, 71839, 46054, 13290,\n",
       "        31082, 72144, 17291, 21776, 25211, 27003, 66340, 57199, 72866,\n",
       "        46281, 70983, 24476, 65543, 40519, 43109, 48288, 65736, 45809,\n",
       "        65543, 30467,  3397, 34885,     1, 38584, 10113, 72024, 72933,\n",
       "          181, 65733, 63328,  3397, 52296, 65543, 71324, 46049, 33556,\n",
       "        50713, 35616, 66340, 63295, 65021, 65543, 20231, 32614, 46281,\n",
       "        72933, 31583, 65683, 38198,  5272, 44626, 66340, 20769, 69549,\n",
       "        46049, 65543, 62790,  7681,  7681,  8855, 71943, 31583, 73569,\n",
       "        71839, 26656,     1,  6005, 66340, 38153, 46049, 65543, 62790,\n",
       "        24476],\n",
       "       [ 1288, 62117,  3397,     1,  7768, 44559, 39012, 49383, 33665,\n",
       "        38216, 69549, 72586, 69952, 47210, 63328,  5709, 66340, 54889,\n",
       "         8824, 48799, 62377, 34001, 62390, 72144, 33665, 24288, 65580,\n",
       "            1,  8634, 45809, 70701, 56131, 66340, 30149, 21543, 32081,\n",
       "        50618, 45809, 33768,  5599, 46169, 66340, 65543, 51584,  3397,\n",
       "            1, 43651,  1737, 46049,  6978, 33665, 62377, 15635, 34807,\n",
       "        35224, 52181, 29692, 60856, 65543, 39012, 34340, 64548, 45812,\n",
       "         3397, 49397,  8855, 41701,  1251, 65543, 49383, 33665, 29890,\n",
       "        33894,  8937, 65543, 12023, 49123, 10480, 55282, 24783, 30149,\n",
       "        67982,   361, 56131,  4783, 42797, 40121, 72381, 41638, 47808,\n",
       "        72144, 35987, 65543, 47761, 14511, 46592, 72586, 60000, 25909,\n",
       "        65683],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,  8073, 46756,   528,  8937,\n",
       "        37539,  2385, 71405,  5984, 18984, 30286, 36557, 31424, 29156,\n",
       "        21744, 57783,  2176, 38665, 57048, 32081, 11925, 71344,  3051,\n",
       "        57682, 66340, 45030, 65543, 13826, 46049, 22488, 33665,     1,\n",
       "        11650,  3397, 27109,  3397,  2642, 32081,  6654, 56227, 65543,\n",
       "        64533, 46049, 37113, 33665,  1737, 63610,  1030,  5599,   410,\n",
       "        45809],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        65773, 23548, 36517, 60828, 31424, 14048, 64394, 51887, 43774,\n",
       "        23666, 46049,  3689, 23768, 10635, 46049, 65543, 47667, 45809,\n",
       "        65543, 37178,   568, 65773, 32520, 67051, 66340, 36514, 45809,\n",
       "        10869, 71992, 58598, 58510, 65543, 57436, 72586, 29692, 37178,\n",
       "        39682, 21724, 65543, 55490, 57048,  9236,   519,  3397,  5599,\n",
       "        41376],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0, 65773, 33665, 19767, 65543, 43109,\n",
       "        68593, 23548, 32786, 65543,  8242,  9408, 63802, 33810, 24003,\n",
       "        33768, 18431, 45177, 26653,     1, 52990, 70555, 45809, 30468,\n",
       "        69054],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0, 60976, 21773, 31424, 36007, 65773, 33665, 63764,\n",
       "        66340,  5272,  2085,  3304, 23548,  8855, 72985, 65683, 59021,\n",
       "        29156, 28696, 46592, 28216,  3689, 65543, 57442, 60569, 48288,\n",
       "        14046,  6873, 65571,  7740, 46592,  2176, 45177, 71476,  1778,\n",
       "        65543, 57043, 16814,  2176, 48896, 17572, 71425, 21922, 65773,\n",
       "        62632, 33665, 66541, 47191, 66340, 71476, 65543,   212, 45809,\n",
       "            1],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0, 65773, 33665,\n",
       "        45177, 65543, 68025, 41140,  8242, 23548, 33768, 71425, 43370,\n",
       "        37545, 59928, 65500, 43109, 45809, 30149, 43290,  2176,   578,\n",
       "        28404],\n",
       "       [71992, 31424, 71425, 38141, 43774, 47578, 66544, 40892,  1697,\n",
       "        66340, 65543, 65547, 66340, 57754, 33178, 33768, 71425, 46065,\n",
       "        45809, 39921, 43290, 31424, 71483, 72586, 43774, 47578,  8855,\n",
       "        65773, 71425, 65543, 46095, 46065, 71596, 71215, 46592, 45809,\n",
       "        59532, 65596, 31424, 28404, 44548, 57783, 33178, 69455, 34885,\n",
       "        53168,  2176, 31424, 14046, 29156, 38154, 46592, 65543, 54471,\n",
       "        45809, 43774, 37775, 72610, 33768, 71943,     1, 50753, 49924,\n",
       "         2176, 47193,  7467, 49032, 45809, 56131, 72443,  2176, 10825,\n",
       "        67419, 72807,  1598, 33665, 46065, 45809, 43774, 23010, 17585,\n",
       "         8855, 33178, 33665,  8937, 22791, 65543, 72952, 49032, 45809,\n",
       "        14332, 45809, 30149,  9611, 32081, 65543, 69110, 63077, 45809,\n",
       "        32692]])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "VaSedeTj-9Gz",
    "outputId": "da2dec73-9e95-45b4-9698-539387ae7ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "iLPwFxHSEXub",
    "outputId": "d90e55f5-ccdc-4b6b-9aac-1d0d073d318e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAJCCAYAAABqEQuqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3pXl4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Distribution of positive and negative reviews\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(labels,kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnEQSvJu_BPI"
   },
   "outputs": [],
   "source": [
    "#single layer\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "embed_size = 300 \n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    #lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a11w4NuDAxEg"
   },
   "outputs": [],
   "source": [
    "#multi-layer\n",
    "lstm_size = 256\n",
    "lstm_layers = 2\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "embed_size = 300 \n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    #drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    #cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(num_units=lstm_size) for _ in range(lstm_layers)])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ic3EtqR_dlC"
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "q2SG4uOz_hVs",
    "outputId": "8a72ce7a-6f20-4101-cdfc-be537b872255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.235\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.238\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.210\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.181\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.250\n",
      "Val acc: 0.539\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.241\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.227\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.230\n",
      "Epoch: 1/10 Iteration: 45 Train loss: 0.193\n",
      "Epoch: 1/10 Iteration: 50 Train loss: 0.150\n",
      "Val acc: 0.744\n",
      "Epoch: 1/10 Iteration: 55 Train loss: 0.108\n",
      "Epoch: 1/10 Iteration: 60 Train loss: 0.102\n",
      "Epoch: 1/10 Iteration: 65 Train loss: 0.116\n",
      "Epoch: 1/10 Iteration: 70 Train loss: 0.135\n",
      "Epoch: 1/10 Iteration: 75 Train loss: 0.125\n",
      "Val acc: 0.671\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.154\n",
      "Epoch: 2/10 Iteration: 85 Train loss: 0.196\n",
      "Epoch: 2/10 Iteration: 90 Train loss: 0.239\n",
      "Epoch: 2/10 Iteration: 95 Train loss: 0.186\n",
      "Epoch: 2/10 Iteration: 100 Train loss: 0.144\n",
      "Val acc: 0.771\n",
      "Epoch: 2/10 Iteration: 105 Train loss: 0.151\n",
      "Epoch: 2/10 Iteration: 110 Train loss: 0.140\n",
      "Epoch: 2/10 Iteration: 115 Train loss: 0.145\n",
      "Epoch: 2/10 Iteration: 120 Train loss: 0.162\n",
      "Epoch: 3/10 Iteration: 125 Train loss: 0.130\n",
      "Val acc: 0.732\n",
      "Epoch: 3/10 Iteration: 130 Train loss: 0.158\n",
      "Epoch: 3/10 Iteration: 135 Train loss: 0.144\n",
      "Epoch: 3/10 Iteration: 140 Train loss: 0.114\n",
      "Epoch: 3/10 Iteration: 145 Train loss: 0.138\n",
      "Epoch: 3/10 Iteration: 150 Train loss: 0.132\n",
      "Val acc: 0.818\n",
      "Epoch: 3/10 Iteration: 155 Train loss: 0.154\n",
      "Epoch: 3/10 Iteration: 160 Train loss: 0.124\n",
      "Epoch: 4/10 Iteration: 165 Train loss: 0.094\n",
      "Epoch: 4/10 Iteration: 170 Train loss: 0.095\n",
      "Epoch: 4/10 Iteration: 175 Train loss: 0.091\n",
      "Val acc: 0.611\n",
      "Epoch: 4/10 Iteration: 180 Train loss: 0.094\n",
      "Epoch: 4/10 Iteration: 185 Train loss: 0.136\n",
      "Epoch: 4/10 Iteration: 190 Train loss: 0.115\n",
      "Epoch: 4/10 Iteration: 195 Train loss: 0.058\n",
      "Epoch: 4/10 Iteration: 200 Train loss: 0.081\n",
      "Val acc: 0.743\n",
      "Epoch: 5/10 Iteration: 205 Train loss: 0.161\n",
      "Epoch: 5/10 Iteration: 210 Train loss: 0.154\n",
      "Epoch: 5/10 Iteration: 215 Train loss: 0.096\n",
      "Epoch: 5/10 Iteration: 220 Train loss: 0.065\n",
      "Epoch: 5/10 Iteration: 225 Train loss: 0.071\n",
      "Val acc: 0.727\n",
      "Epoch: 5/10 Iteration: 230 Train loss: 0.033\n",
      "Epoch: 5/10 Iteration: 235 Train loss: 0.027\n",
      "Epoch: 5/10 Iteration: 240 Train loss: 0.023\n",
      "Epoch: 6/10 Iteration: 245 Train loss: 0.132\n",
      "Epoch: 6/10 Iteration: 250 Train loss: 0.172\n",
      "Val acc: 0.714\n",
      "Epoch: 6/10 Iteration: 255 Train loss: 0.119\n",
      "Epoch: 6/10 Iteration: 260 Train loss: 0.086\n",
      "Epoch: 6/10 Iteration: 265 Train loss: 0.045\n",
      "Epoch: 6/10 Iteration: 270 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 275 Train loss: 0.017\n",
      "Val acc: 0.703\n",
      "Epoch: 6/10 Iteration: 280 Train loss: 0.016\n",
      "Epoch: 7/10 Iteration: 285 Train loss: 0.111\n",
      "Epoch: 7/10 Iteration: 290 Train loss: 0.091\n",
      "Epoch: 7/10 Iteration: 295 Train loss: 0.068\n",
      "Epoch: 7/10 Iteration: 300 Train loss: 0.086\n",
      "Val acc: 0.744\n",
      "Epoch: 7/10 Iteration: 305 Train loss: 0.087\n",
      "Epoch: 7/10 Iteration: 310 Train loss: 0.054\n",
      "Epoch: 7/10 Iteration: 315 Train loss: 0.036\n",
      "Epoch: 7/10 Iteration: 320 Train loss: 0.030\n",
      "Epoch: 8/10 Iteration: 325 Train loss: 0.209\n",
      "Val acc: 0.684\n",
      "Epoch: 8/10 Iteration: 330 Train loss: 0.146\n",
      "Epoch: 8/10 Iteration: 335 Train loss: 0.172\n",
      "Epoch: 8/10 Iteration: 340 Train loss: 0.157\n",
      "Epoch: 8/10 Iteration: 345 Train loss: 0.080\n",
      "Epoch: 8/10 Iteration: 350 Train loss: 0.084\n",
      "Val acc: 0.803\n",
      "Epoch: 8/10 Iteration: 355 Train loss: 0.072\n",
      "Epoch: 8/10 Iteration: 360 Train loss: 0.049\n",
      "Epoch: 9/10 Iteration: 365 Train loss: 0.053\n",
      "Epoch: 9/10 Iteration: 370 Train loss: 0.052\n",
      "Epoch: 9/10 Iteration: 375 Train loss: 0.036\n",
      "Val acc: 0.871\n",
      "Epoch: 9/10 Iteration: 380 Train loss: 0.018\n",
      "Epoch: 9/10 Iteration: 385 Train loss: 0.007\n",
      "Epoch: 9/10 Iteration: 390 Train loss: 0.003\n",
      "Epoch: 9/10 Iteration: 395 Train loss: 0.001\n",
      "Epoch: 9/10 Iteration: 400 Train loss: 0.001\n",
      "Val acc: 0.830\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jZWr5o1g2Hpj",
    "outputId": "d90fe26c-6a1a-40e5-c224-3ac670048050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.790\n"
     ]
    }
   ],
   "source": [
    "#prediction on test set\n",
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tg5RhlosS9mD"
   },
   "source": [
    "### Summary of Parametric Study\n",
    "\n",
    "| LSTM Size \t| 1 LSTM Layer \t| 2 LSTM Layers \t| 3 LSTM Layers \t| 4 LSTM Layers \t|\n",
    "|-----------\t|--------------\t|---------------\t|---------------\t|---------------\t|\n",
    "| 128 \t| 0.59 \t|  \t|  \t|  \t|\n",
    "| 256 \t| 0.78 \t| 0.858 \t| 0.736 \t| 0.50 \t|\n",
    "| 512 \t| 0.71 \t|  \t|  \t|  \t|\n",
    "\n",
    "From the table we can see that  for **LSTM size = 256** with 2 layer deep network gives an accuracy of almost **86%** on the test set"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment_analysis_udacity2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
